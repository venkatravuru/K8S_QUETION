ğŸ”¹ Scenario 1: Pod in CrashLoopBackOff

âœ… Use Case: Your application Pod keeps restarting right after it starts.

â“ Interview Question: You deployed a Pod, but it's stuck in CrashLoopBackOff. What steps do you take to troubleshoot and fix it?

ğŸ’¡ Answer: Check Pod logs:

bash Copy Edit kubectl logs Check if itâ€™s a config issue: Missing env vars, DB unreachable, crash in startup script.

Describe the Pod to view events:

bash Copy Edit kubectl describe pod Check liveness probe settings â€” it may be killing the container too early.

Fix the root issue, update the YAML, and reapply.

================================================

ğŸ”¹ Scenario 2: Pod Stuck in Pending State

âœ… Use Case: You created a Pod, but it's stuck in Pending for several minutes.

â“ Interview Question: A Pod is not scheduling and remains in Pending. What could be causing this, and how would you resolve it?

ğŸ’¡ Answer: Describe the Pod:

bash Copy Edit kubectl describe pod Common causes:

Insufficient node resources (CPU/Memory)

PVC not bound

No matching Tolerations / Affinity issues

Fix it:

Adjust resource requests or limits

Scale the cluster

Modify affinity or taints/tolerations

==========================================

ğŸ”¹ Scenario 8: Pod Can't Pull Image from Private Registry

âœ… Use Case: Your Pod fails with ImagePullBackOff.

â“ Interview Question: What steps would you take if your Pod canâ€™t pull an image from a private registry?

ğŸ’¡ Answer: Create a secret with your registry credentials:

bash Copy Edit kubectl create secret docker-registry my-reg-secret
--docker-username= --docker-password= --docker-server= Reference it in the Pod spec:

yaml Copy Edit imagePullSecrets:

name: my-reg-secret

====================================================

ğŸ”¹ Scenario 9: Sidecar Container to Process Shared Data

âœ… Use Case: You want a second container to process files written by the main app.

â“ Interview Question: How can two containers in the same Pod share data?

ğŸ’¡ Answer: Use emptyDir volume:

yaml Copy Edit volumes:

name: shared emptyDir: {}
containers:

name: main volumeMounts:
name: shared mountPath: /data
name: sidecar volumeMounts:
name: shared mountPath: /data Both containers can read/write to /data.

=========================================================================

ğŸ”¹ BONUS: Multi-Container Pod Interview Question â“ How are multi-container Pods useful? Give a real-world use case. ğŸ’¡ Answer: Use Case: One container runs the app, another logs or watches files.

Example: App writes logs â†’ Sidecar ships them to Fluentd.

Another: App exposes metrics â†’ Sidecar collects and exposes via Prometheus.

=================================================================================

ğŸ”¹ Scenario 11: Pod with High CPU Usage Under Load â“ Question: Your application Pod consumes too much CPU under load. How do you prevent it from affecting other Pods on the same node?

ğŸ’¡ Answer: Use CPU limits in the Pod spec:

yaml Copy Edit resources: requests: cpu: "250m" limits: cpu: "500m" This ensures the Pod doesnâ€™t use more than 500m (0.5 CPU).

If the app still needs more, use HPA to scale out instead of increasing CPU limits.

Scenario 12: App Gets Killed by OOM (Out of Memory) â“ Question: How do you prevent your application from being OOMKilled?

ğŸ’¡ Answer: Use memory limits:

yaml Copy Edit resources: requests: memory: "256Mi" limits: memory: "512Mi" Monitor app usage and set realistic memory requests and limits.

Investigate memory leaks in the app if it grows uncontrollably.

Scenario 13: Pod Health Checks Are Failing â“ Question: What types of health checks does Kubernetes support for Pods, and how are they different?

ğŸ’¡ Answer: Kubernetes supports 3 types of probes:

Liveness Probe â€“ checks if the app should be restarted.

Readiness Probe â€“ checks if the app is ready to serve traffic.

Startup Probe â€“ waits for the app to start before applying other probes.

Use:

yaml Copy Edit readinessProbe: httpGet: path: /ready port: 8080

===============================================================

ğŸ”¹ Scenario 14: Need to Debug a Running Pod
â“ Question: How can you troubleshoot a running Pod if you can't SSH into it?

ğŸ’¡ Answer: Use kubectl exec to run a command inside the Pod:

bash Copy Edit kubectl exec -it -- /bin/sh If no shell is available, run a debug container:

bash Copy Edit kubectl debug -it --image=busybox --target=


===================================================================

ğŸ”¹ Scenario 15: Need to Restart a Single Pod â“ Question: How do you restart a specific Pod?

ğŸ’¡ Answer: Pods can't be restarted directly (stateless). Instead:

bash Copy Edit kubectl delete pod The Deployment or ReplicaSet will create a new one automatically.

Scenario 16: Pod Lifecycle Management â“ Question: What are the lifecycle states of a Kubernetes Pod?

ğŸ’¡ Answer: Pending: Pod accepted but not running yet

Running: Pod is up and containers are running

Succeeded: Pod completed (for jobs)

Failed: Pod terminated with failure

Unknown: Node not reachable

Use kubectl get pod -o wide or kubectl describe pod to check status.

=======================================================================



ğŸ“˜ Pod Lifecycle in Sequence (Simple Story)
You create a pod (e.g., via kubectl apply).

Pod goes to Pending while Kubernetes:

Picks a node

Pulls the image

Attaches PVCs

Once ready, pod moves to Running.
                         -------

If the containers exit:

Successfully Pod becomes Succeeded
------------
With error â†’ Pod becomes Failed
                        ---------

If the pod is deleted â†’ containers are terminated, and Kubernetes may restart the pod if part of a controller (e.g. Deployment).



===========================================================================================


ğŸ”¹ 1. Pod in CrashLoopBackOff
Q:

Your pod keeps restarting and shows CrashLoopBackOff. How would you fix it?

A:

"First, Iâ€™d run kubectl describe pod <name> to check recent events â€” especially around container starts or probe failures.
Then I'd run kubectl logs <pod-name> to see application errors. Common causes are missing env variables, crash in entrypoint, or failing health checks. Based on what I find, Iâ€™d update the deployment or config and rollout a fix."

ğŸ”¹ 2. Pod in Pending
Q:

Your pod stays in Pending state. What could cause that?

A:

"Usually, it's a scheduling issue. I check kubectl describe pod to see if there's a message like 'no nodes available' or 'insufficient resources'.
If it's a PVC problem, the pod can also get stuck â€” so Iâ€™d check whether the PVC is bound."

ğŸ”¹ 3. Pod Can't Start Due to Volume Mount Error
Q:

Your pod fails with a volume mount error. How do you troubleshoot?

A:

"Iâ€™d check events in kubectl describe pod â€” common issues are wrong NFS paths, unreachable storage servers, or access issues.
Iâ€™d also check whether the PVC is bound to a matching PV and whether the access mode is correct."

ğŸ”¹ 4. Pod Networking Issue
Q:

One pod canâ€™t reach another pod. How do you debug this?

A:

"First, Iâ€™d check if both pods are in the same namespace and have a valid DNS entry. Then Iâ€™d use kubectl exec into the source pod and test with curl or ping.
Iâ€™d verify the target pod is running and Ready. If itâ€™s a Service issue, Iâ€™d check selectors, endpoints, and port exposure."

ğŸ”¹ 5. Container Stuck in ImagePullBackOff
Q:

The pod fails to start due to an image issue. What are the steps?

A:

"Iâ€™d check if the image name is correct and if authentication is needed for a private registry.
Iâ€™d look at kubectl describe pod â€” if it says ImagePullBackOff, it may be a typo or missing pull secret."

ğŸ”¹ 6. Multiple Containers in One Pod (Sidecar Scenario)
Q:

Why and how would you run multiple containers in a single pod?

A:

"Sometimes you need tightly coupled containers to share volumes or network â€” like a log forwarder (sidecar) or a file init container.
Since containers in a pod share the same network namespace and volumes, it makes communication easy."

ğŸ”¹ 7. Liveness vs Readiness Probes
Q:

Your pod restarts frequently due to liveness probe failure. How do you fix it?

A:

"Iâ€™d check if the liveness probe is too aggressive â€” maybe the app isnâ€™t ready yet.
Iâ€™d increase initialDelaySeconds, or adjust the probe path/port.
If the container isnâ€™t crashing, but just not Ready, Iâ€™d review the readiness probe instead."

ğŸ”¹ 9. Pod Not Restarting Automatically
Q:

Your pod crashed but didnâ€™t restart. Why?

A:

"If the pod was created standalone (not through a Deployment), it won't restart unless you recreate it.
If it's in a Job or a completed state (Succeeded), that's expected.
Also, Iâ€™d check the restartPolicy â€” Never, OnFailure, or Always."

ğŸ”¹ 10. Init Container Use Case
Q:

When would you use an initContainer in a pod?

A:

"Iâ€™d use it when I need to prepare something before the main app starts â€” like waiting for a service, copying files, or setting permissions.
Init containers run sequentially, before the main app starts."

ğŸ”¹ 11. Pod Reaches Succeeded and Exits
Q:

Your pod runs a script and stops with Completed (Succeeded). Is that normal?

A:

"Yes â€” if the containerâ€™s process ends successfully (exit code 0), Kubernetes marks the pod as Succeeded.
This is typical for Jobs or one-time batch tasks."

ğŸ”¹ 12. Pod Access Control
Q:

How do you prevent a pod from accessing resources in another namespace?

A:

"Iâ€™d use Kubernetes RBAC to scope the service account permissions and NetworkPolicies to limit cross-namespace traffic.
By default, services in one namespace cannot talk to others unless allowed."

ğŸ”¹ 13. Pod Logs Are Missing
Q:

You run kubectl logs <pod> and get an error. Why?

A:

"The pod might be terminated or hasn't started yet. For multi-container pods, I need to specify -c <container-name>.
If it was deleted, logs are gone unless a logging system (like EFK) is used."
