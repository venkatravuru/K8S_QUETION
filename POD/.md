🔹 Scenario 1: Pod in CrashLoopBackOff

✅ Use Case: Your application Pod keeps restarting right after it starts.

❓ Interview Question: You deployed a Pod, but it's stuck in CrashLoopBackOff. What steps do you take to troubleshoot and fix it?

💡 Answer: Check Pod logs:

bash Copy Edit kubectl logs Check if it’s a config issue: Missing env vars, DB unreachable, crash in startup script.

Describe the Pod to view events:

bash Copy Edit kubectl describe pod Check liveness probe settings — it may be killing the container too early.

Fix the root issue, update the YAML, and reapply.

================================================

🔹 Scenario 2: Pod Stuck in Pending State

✅ Use Case: You created a Pod, but it's stuck in Pending for several minutes.

❓ Interview Question: A Pod is not scheduling and remains in Pending. What could be causing this, and how would you resolve it?

💡 Answer: Describe the Pod:

bash Copy Edit kubectl describe pod Common causes:

Insufficient node resources (CPU/Memory)

PVC not bound

No matching Tolerations / Affinity issues

Fix it:

Adjust resource requests or limits

Scale the cluster

Modify affinity or taints/tolerations

==========================================

🔹 Scenario 8: Pod Can't Pull Image from Private Registry

✅ Use Case: Your Pod fails with ImagePullBackOff.

❓ Interview Question: What steps would you take if your Pod can’t pull an image from a private registry?

💡 Answer: Create a secret with your registry credentials:

bash Copy Edit kubectl create secret docker-registry my-reg-secret
--docker-username= --docker-password= --docker-server= Reference it in the Pod spec:

yaml Copy Edit imagePullSecrets:

name: my-reg-secret

====================================================

🔹 Scenario 9: Sidecar Container to Process Shared Data

✅ Use Case: You want a second container to process files written by the main app.

❓ Interview Question: How can two containers in the same Pod share data?

💡 Answer: Use emptyDir volume:

yaml Copy Edit volumes:

name: shared emptyDir: {}
containers:

name: main volumeMounts:
name: shared mountPath: /data
name: sidecar volumeMounts:
name: shared mountPath: /data Both containers can read/write to /data.

=========================================================================

🔹 BONUS: Multi-Container Pod Interview Question ❓ How are multi-container Pods useful? Give a real-world use case. 💡 Answer: Use Case: One container runs the app, another logs or watches files.

Example: App writes logs → Sidecar ships them to Fluentd.

Another: App exposes metrics → Sidecar collects and exposes via Prometheus.

=================================================================================

🔹 Scenario 11: Pod with High CPU Usage Under Load ❓ Question: Your application Pod consumes too much CPU under load. How do you prevent it from affecting other Pods on the same node?

💡 Answer: Use CPU limits in the Pod spec:

yaml Copy Edit resources: requests: cpu: "250m" limits: cpu: "500m" This ensures the Pod doesn’t use more than 500m (0.5 CPU).

If the app still needs more, use HPA to scale out instead of increasing CPU limits.

Scenario 12: App Gets Killed by OOM (Out of Memory) ❓ Question: How do you prevent your application from being OOMKilled?

💡 Answer: Use memory limits:

yaml Copy Edit resources: requests: memory: "256Mi" limits: memory: "512Mi" Monitor app usage and set realistic memory requests and limits.

Investigate memory leaks in the app if it grows uncontrollably.

Scenario 13: Pod Health Checks Are Failing ❓ Question: What types of health checks does Kubernetes support for Pods, and how are they different?

💡 Answer: Kubernetes supports 3 types of probes:

Liveness Probe – checks if the app should be restarted.

Readiness Probe – checks if the app is ready to serve traffic.

Startup Probe – waits for the app to start before applying other probes.

Use:

yaml Copy Edit readinessProbe: httpGet: path: /ready port: 8080

===============================================================

🔹 Scenario 14: Need to Debug a Running Pod
❓ Question: How can you troubleshoot a running Pod if you can't SSH into it?

💡 Answer: Use kubectl exec to run a command inside the Pod:

bash Copy Edit kubectl exec -it -- /bin/sh If no shell is available, run a debug container:

bash Copy Edit kubectl debug -it --image=busybox --target=


===================================================================

🔹 Scenario 15: Need to Restart a Single Pod ❓ Question: How do you restart a specific Pod?

💡 Answer: Pods can't be restarted directly (stateless). Instead:

bash Copy Edit kubectl delete pod The Deployment or ReplicaSet will create a new one automatically.

Scenario 16: Pod Lifecycle Management ❓ Question: What are the lifecycle states of a Kubernetes Pod?

💡 Answer: Pending: Pod accepted but not running yet

Running: Pod is up and containers are running

Succeeded: Pod completed (for jobs)

Failed: Pod terminated with failure

Unknown: Node not reachable

Use kubectl get pod -o wide or kubectl describe pod to check status.

=======================================================================



📘 Pod Lifecycle in Sequence (Simple Story)
You create a pod (e.g., via kubectl apply).

Pod goes to Pending while Kubernetes:

Picks a node

Pulls the image

Attaches PVCs

Once ready, pod moves to Running.
                         -------

If the containers exit:

Successfully Pod becomes Succeeded
------------
With error → Pod becomes Failed
                        ---------

If the pod is deleted → containers are terminated, and Kubernetes may restart the pod if part of a controller (e.g. Deployment).



===========================================================================================


🔹 1. Pod in CrashLoopBackOff
Q:

Your pod keeps restarting and shows CrashLoopBackOff. How would you fix it?

A:

"First, I’d run kubectl describe pod <name> to check recent events — especially around container starts or probe failures.
Then I'd run kubectl logs <pod-name> to see application errors. Common causes are missing env variables, crash in entrypoint, or failing health checks. Based on what I find, I’d update the deployment or config and rollout a fix."

🔹 2. Pod in Pending
Q:

Your pod stays in Pending state. What could cause that?

A:

"Usually, it's a scheduling issue. I check kubectl describe pod to see if there's a message like 'no nodes available' or 'insufficient resources'.
If it's a PVC problem, the pod can also get stuck — so I’d check whether the PVC is bound."

🔹 3. Pod Can't Start Due to Volume Mount Error
Q:

Your pod fails with a volume mount error. How do you troubleshoot?

A:

"I’d check events in kubectl describe pod — common issues are wrong NFS paths, unreachable storage servers, or access issues.
I’d also check whether the PVC is bound to a matching PV and whether the access mode is correct."

🔹 4. Pod Networking Issue
Q:

One pod can’t reach another pod. How do you debug this?

A:

"First, I’d check if both pods are in the same namespace and have a valid DNS entry. Then I’d use kubectl exec into the source pod and test with curl or ping.
I’d verify the target pod is running and Ready. If it’s a Service issue, I’d check selectors, endpoints, and port exposure."

🔹 5. Container Stuck in ImagePullBackOff
Q:

The pod fails to start due to an image issue. What are the steps?

A:

"I’d check if the image name is correct and if authentication is needed for a private registry.
I’d look at kubectl describe pod — if it says ImagePullBackOff, it may be a typo or missing pull secret."

🔹 6. Multiple Containers in One Pod (Sidecar Scenario)
Q:

Why and how would you run multiple containers in a single pod?

A:

"Sometimes you need tightly coupled containers to share volumes or network — like a log forwarder (sidecar) or a file init container.
Since containers in a pod share the same network namespace and volumes, it makes communication easy."

🔹 7. Liveness vs Readiness Probes
Q:

Your pod restarts frequently due to liveness probe failure. How do you fix it?

A:

"I’d check if the liveness probe is too aggressive — maybe the app isn’t ready yet.
I’d increase initialDelaySeconds, or adjust the probe path/port.
If the container isn’t crashing, but just not Ready, I’d review the readiness probe instead."

🔹 9. Pod Not Restarting Automatically
Q:

Your pod crashed but didn’t restart. Why?

A:

"If the pod was created standalone (not through a Deployment), it won't restart unless you recreate it.
If it's in a Job or a completed state (Succeeded), that's expected.
Also, I’d check the restartPolicy — Never, OnFailure, or Always."

🔹 10. Init Container Use Case
Q:

When would you use an initContainer in a pod?

A:

"I’d use it when I need to prepare something before the main app starts — like waiting for a service, copying files, or setting permissions.
Init containers run sequentially, before the main app starts."

🔹 11. Pod Reaches Succeeded and Exits
Q:

Your pod runs a script and stops with Completed (Succeeded). Is that normal?

A:

"Yes — if the container’s process ends successfully (exit code 0), Kubernetes marks the pod as Succeeded.
This is typical for Jobs or one-time batch tasks."

🔹 12. Pod Access Control
Q:

How do you prevent a pod from accessing resources in another namespace?

A:

"I’d use Kubernetes RBAC to scope the service account permissions and NetworkPolicies to limit cross-namespace traffic.
By default, services in one namespace cannot talk to others unless allowed."

🔹 13. Pod Logs Are Missing
Q:

You run kubectl logs <pod> and get an error. Why?

A:

"The pod might be terminated or hasn't started yet. For multi-container pods, I need to specify -c <container-name>.
If it was deleted, logs are gone unless a logging system (like EFK) is used."
